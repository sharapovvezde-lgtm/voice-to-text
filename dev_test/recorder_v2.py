"""
Meeting Recorder v2 ‚Äî –ó–∞–ø–∏—Å—å —ç–∫—Ä–∞–Ω–∞ —Å–æ –∑–≤—É–∫–æ–º
- –í–∏–¥–µ–æ: –∑–∞—Ö–≤–∞—Ç –¢–û–ß–ù–û–ô –≤—ã–±—Ä–∞–Ω–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ —ç–∫—Ä–∞–Ω–∞
- –ê—É–¥–∏–æ: –º–∏–∫—Ä–æ—Ñ–æ–Ω (–Ø) + —Å–∏—Å—Ç–µ–º–Ω—ã–π –∑–≤—É–∫ (–°–æ–±–µ—Å–µ–¥–Ω–∏–∫) 
- –í—ã—Ö–æ–¥: MP4 —Å–æ –∑–≤—É–∫–æ–º + WAV —Ñ–∞–π–ª—ã –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏
"""
import os
import sys
import time
import threading
import wave
from datetime import datetime
from pathlib import Path

import numpy as np
import cv2
import mss
import sounddevice as sd

# ===== –í–∏–¥–∂–µ—Ç –≤—ã–±–æ—Ä–∞ –æ–±–ª–∞—Å—Ç–∏ —ç–∫—Ä–∞–Ω–∞ =====
from PyQt6.QtWidgets import QWidget, QApplication
from PyQt6.QtCore import Qt, QRect, QPoint
from PyQt6.QtGui import QPainter, QColor, QFont, QPen


class ScreenRegionSelector(QWidget):
    """–ü–æ–ª–Ω–æ—ç–∫—Ä–∞–Ω–Ω—ã–π –≤–∏–¥–∂–µ—Ç –¥–ª—è –≤—ã–±–æ—Ä–∞ –æ–±–ª–∞—Å—Ç–∏ –∑–∞–ø–∏—Å–∏"""
    
    def __init__(self, callback=None):
        super().__init__()
        self.callback = callback
        self.selection = None
        self._drawing = False
        
        # –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –Ω–∞—á–∞–ª–∞ –∏ –∫–æ–Ω—Ü–∞ –≤—ã–¥–µ–ª–µ–Ω–∏—è
        self._start_global = QPoint()
        self._end_global = QPoint()
        
        self.setWindowFlags(
            Qt.WindowType.FramelessWindowHint |
            Qt.WindowType.WindowStaysOnTopHint |
            Qt.WindowType.BypassWindowManagerHint
        )
        self.setAttribute(Qt.WidgetAttribute.WA_TranslucentBackground, True)
        self.setAttribute(Qt.WidgetAttribute.WA_DeleteOnClose, False)
        self.setCursor(Qt.CursorShape.CrossCursor)
        self._setup_geometry()
    
    def _setup_geometry(self):
        screens = QApplication.screens()
        if not screens:
            return
        min_x = min(s.geometry().x() for s in screens)
        min_y = min(s.geometry().y() for s in screens)
        max_x = max(s.geometry().x() + s.geometry().width() for s in screens)
        max_y = max(s.geometry().y() + s.geometry().height() for s in screens)
        self._virtual_x = min_x
        self._virtual_y = min_y
        self.setGeometry(min_x, min_y, max_x - min_x, max_y - min_y)
    
    def showFullScreen(self):
        self._setup_geometry()
        self.show()
        self.raise_()
        self.activateWindow()
        self.setFocus()
    
    def _global_to_local(self, global_point):
        return QPoint(global_point.x() - self._virtual_x, global_point.y() - self._virtual_y)
    
    def _get_selection_rect_local(self):
        start_local = self._global_to_local(self._start_global)
        end_local = self._global_to_local(self._end_global)
        return QRect(start_local, end_local).normalized()
    
    def paintEvent(self, event):
        painter = QPainter(self)
        painter.setRenderHint(QPainter.RenderHint.Antialiasing)
        painter.fillRect(self.rect(), QColor(0, 0, 0, 150))
        
        if self._drawing:
            local_rect = self._get_selection_rect_local()
            if local_rect.width() > 5 and local_rect.height() > 5:
                painter.setCompositionMode(QPainter.CompositionMode.CompositionMode_Clear)
                painter.fillRect(local_rect, Qt.GlobalColor.transparent)
                painter.setCompositionMode(QPainter.CompositionMode.CompositionMode_SourceOver)
                
                pen = QPen(QColor(0, 255, 0), 3)
                painter.setPen(pen)
                painter.drawRect(local_rect)
                
                size_text = f"{local_rect.width()} √ó {local_rect.height()}  üìç({self._start_global.x()}, {self._start_global.y()})"
                painter.setFont(QFont("Arial", 16, QFont.Weight.Bold))
                painter.setPen(QColor(255, 255, 0))
                text_y = local_rect.y() - 10
                if text_y < 25:
                    text_y = local_rect.bottom() + 25
                painter.drawText(local_rect.x() + 5, text_y, size_text)
        
        painter.setPen(QColor(255, 255, 255))
        painter.setFont(QFont("Arial", 20, QFont.Weight.Bold))
        painter.drawText(self.rect().adjusted(0, 50, 0, 0), 
                        Qt.AlignmentFlag.AlignTop | Qt.AlignmentFlag.AlignHCenter,
                        "üéØ –ó–ê–ñ–ú–ò–¢–ï –ª–µ–≤—É—é –∫–Ω–æ–ø–∫—É –º—ã—à–∏ –∏ –≤—ã–¥–µ–ª–∏—Ç–µ –æ–±–ª–∞—Å—Ç—å")
        painter.setFont(QFont("Arial", 14))
        painter.drawText(self.rect().adjusted(0, 90, 0, 0),
                        Qt.AlignmentFlag.AlignTop | Qt.AlignmentFlag.AlignHCenter,
                        "ESC = –æ—Ç–º–µ–Ω–∞")
    
    def mousePressEvent(self, event):
        if event.button() == Qt.MouseButton.LeftButton:
            global_pos = event.globalPosition().toPoint()
            self._start_global = global_pos
            self._end_global = global_pos
            self._drawing = True
            self.update()
    
    def mouseMoveEvent(self, event):
        if self._drawing:
            self._end_global = event.globalPosition().toPoint()
            self.update()
    
    def mouseReleaseEvent(self, event):
        if event.button() == Qt.MouseButton.LeftButton and self._drawing:
            self._drawing = False
            self._end_global = event.globalPosition().toPoint()
            
            x1 = min(self._start_global.x(), self._end_global.x())
            y1 = min(self._start_global.y(), self._end_global.y())
            x2 = max(self._start_global.x(), self._end_global.x())
            y2 = max(self._start_global.y(), self._end_global.y())
            
            width = x2 - x1
            height = y2 - y1
            
            if width >= 50 and height >= 50:
                global_rect = {
                    "left": x1,
                    "top": y1,
                    "width": width,
                    "height": height
                }
                print(f"üéØ –í—ã–±—Ä–∞–Ω–∞ –æ–±–ª–∞—Å—Ç—å: left={x1}, top={y1}, width={width}, height={height}")
                self.selection = global_rect
                self.hide()
                if self.callback:
                    self.callback(global_rect)
            else:
                self._start_global = QPoint()
                self._end_global = QPoint()
                self.update()
    
    def keyPressEvent(self, event):
        if event.key() == Qt.Key.Key_Escape:
            self.selection = None
            self.hide()
            if self.callback:
                self.callback(None)


class MeetingRecorder:
    """–ó–∞–ø–∏—Å—å –≤—Å—Ç—Ä–µ—á: —ç–∫—Ä–∞–Ω + –º–∏–∫—Ä–æ—Ñ–æ–Ω + —Å–∏—Å—Ç–µ–º–Ω—ã–π –∑–≤—É–∫"""
    
    def __init__(self, output_dir: str = None):
        self.output_dir = Path(output_dir) if output_dir else Path("./records")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.fps = 15
        self.audio_rate = 44100
        self.monitor = None
        self.mic_device = None
        
        self.is_recording = False
        self._stop_event = threading.Event()
        
        # –ë—É—Ñ–µ—Ä—ã
        self._video_frames = []
        self._mic_audio_data = []
        self._sys_audio_data = []
        
        # –ü–æ—Ç–æ–∫–∏
        self._video_thread = None
        self._mic_thread = None
        self._sys_thread = None
        
        self._loopback_device = None
    
    def get_monitors(self) -> list:
        with mss.mss() as sct:
            monitors = []
            for i, mon in enumerate(sct.monitors):
                if i == 0:
                    continue
                monitors.append({
                    "id": i, "name": f"–ú–æ–Ω–∏—Ç–æ—Ä {i}",
                    "width": mon["width"], "height": mon["height"],
                    "left": mon["left"], "top": mon["top"]
                })
            return monitors
    
    def get_microphones(self) -> list:
        devices = sd.query_devices()
        mics = []
        for i, dev in enumerate(devices):
            if dev['max_input_channels'] > 0:
                mics.append({
                    "id": i, "name": dev['name'],
                    "channels": dev['max_input_channels'],
                    "is_default": i == sd.default.device[0]
                })
        return mics
    
    def get_loopback_device(self):
        """–ù–∞–π—Ç–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∑–≤—É–∫–∞"""
        try:
            import pyaudiowpatch as pyaudio
            p = pyaudio.PyAudio()
            
            for i in range(p.get_device_count()):
                dev = p.get_device_info_by_index(i)
                if dev.get('isLoopbackDevice', False):
                    print(f"üîä –ù–∞–π–¥–µ–Ω Loopback: {dev['name']}")
                    p.terminate()
                    return dev
            
            p.terminate()
            print("‚ö†Ô∏è Loopback —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        except ImportError:
            print("‚ö†Ô∏è pyaudiowpatch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ loopback: {e}")
        return None
    
    def _record_video(self):
        """–ü–æ—Ç–æ–∫ –∑–∞–ø–∏—Å–∏ –≤–∏–¥–µ–æ ‚Äî –¢–û–ß–ù–ê–Ø –æ–±–ª–∞—Å—Ç—å!"""
        print(f"üìπ –í–∏–¥–µ–æ: —Å—Ç–∞—Ä—Ç")
        print(f"   –û–±–ª–∞—Å—Ç—å: left={self.monitor['left']}, top={self.monitor['top']}, "
              f"w={self.monitor['width']}, h={self.monitor['height']}")
        
        first_frame = True
        with mss.mss() as sct:
            frame_time = 1.0 / self.fps
            
            # –í–ê–ñ–ù–û: –°–æ–∑–¥–∞—ë–º –∫–æ–ø–∏—é —Å–ª–æ–≤–∞—Ä—è –¥–ª—è mss
            grab_region = {
                "left": int(self.monitor['left']),
                "top": int(self.monitor['top']),
                "width": int(self.monitor['width']),
                "height": int(self.monitor['height'])
            }
            
            while not self._stop_event.is_set():
                start = time.time()
                try:
                    # –ó–∞—Ö–≤–∞—Ç—ã–≤–∞–µ–º –¢–û–ß–ù–û —É–∫–∞–∑–∞–Ω–Ω—É—é –æ–±–ª–∞—Å—Ç—å
                    img = sct.grab(grab_region)
                    frame = np.array(img)
                    
                    if first_frame:
                        print(f"   –ü–µ—Ä–≤—ã–π –∫–∞–¥—Ä: {frame.shape[1]}x{frame.shape[0]} px")
                        first_frame = False
                    
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGRA2BGR)
                    self._video_frames.append(frame)
                except Exception as e:
                    print(f"Video err: {e}")
                
                elapsed = time.time() - start
                if elapsed < frame_time:
                    time.sleep(frame_time - elapsed)
        
        print(f"üìπ –í–∏–¥–µ–æ: {len(self._video_frames)} –∫–∞–¥—Ä–æ–≤")
    
    def _record_microphone(self):
        """–ü–æ—Ç–æ–∫ –∑–∞–ø–∏—Å–∏ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞"""
        print(f"üé§ –ú–∏–∫—Ä–æ—Ñ–æ–Ω: —Å—Ç–∞—Ä—Ç (—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ {self.mic_device})")
        
        try:
            chunk_samples = int(self.audio_rate * 0.1)
            
            stream = sd.InputStream(
                device=self.mic_device,
                samplerate=self.audio_rate,
                channels=1,
                dtype='int16',
                blocksize=chunk_samples
            )
            stream.start()
            
            while not self._stop_event.is_set():
                try:
                    data, _ = stream.read(chunk_samples)
                    # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –¥–∞–Ω–Ω—ã–µ 1D
                    flat_data = data.flatten().copy()
                    self._mic_audio_data.append(flat_data)
                except Exception as e:
                    print(f"Mic read err: {e}")
                    time.sleep(0.05)
            
            stream.stop()
            stream.close()
            print(f"üé§ –ú–∏–∫—Ä–æ—Ñ–æ–Ω: {len(self._mic_audio_data)} —á–∞–Ω–∫–æ–≤")
            
        except Exception as e:
            print(f"‚ùå –ú–∏–∫—Ä–æ—Ñ–æ–Ω –æ—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
    
    def _record_system_audio(self):
        """–ü–æ—Ç–æ–∫ –∑–∞–ø–∏—Å–∏ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∑–≤—É–∫–∞ —á–µ—Ä–µ–∑ WASAPI Loopback"""
        if not self._loopback_device:
            print("‚ö†Ô∏è –°–∏—Å—Ç–µ–º–Ω—ã–π –∑–≤—É–∫: –ø—Ä–æ–ø—É—â–µ–Ω")
            return
        
        print(f"üîä –°–∏—Å—Ç–µ–º–Ω—ã–π –∑–≤—É–∫: —Å—Ç–∞—Ä—Ç")
        
        try:
            import pyaudiowpatch as pyaudio
            p = pyaudio.PyAudio()
            
            device_index = self._loopback_device['index']
            channels = int(self._loopback_device['maxInputChannels'])
            rate = int(self._loopback_device['defaultSampleRate'])
            
            stream = p.open(
                format=pyaudio.paInt16,
                channels=channels,
                rate=rate,
                input=True,
                input_device_index=device_index,
                frames_per_buffer=int(rate * 0.1)
            )
            
            while not self._stop_event.is_set():
                try:
                    data = stream.read(int(rate * 0.1), exception_on_overflow=False)
                    audio_data = np.frombuffer(data, dtype=np.int16)
                    
                    # –ï—Å–ª–∏ —Å—Ç–µ—Ä–µ–æ - –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –º–æ–Ω–æ
                    if channels > 1:
                        audio_data = audio_data.reshape(-1, channels)
                        audio_data = np.mean(audio_data, axis=1).astype(np.int16)
                    
                    # –†–µ—Å–µ–º–ø–ª–∏—Ä—É–µ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    if rate != self.audio_rate:
                        ratio = self.audio_rate / rate
                        new_len = int(len(audio_data) * ratio)
                        indices = np.linspace(0, len(audio_data) - 1, new_len).astype(int)
                        audio_data = audio_data[indices]
                    
                    # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –¥–∞–Ω–Ω—ã–µ 1D
                    self._sys_audio_data.append(audio_data.flatten().copy())
                except Exception as e:
                    print(f"Sys read err: {e}")
                    time.sleep(0.05)
            
            stream.stop_stream()
            stream.close()
            p.terminate()
            print(f"üîä –°–∏—Å—Ç–µ–º–Ω—ã–π –∑–≤—É–∫: {len(self._sys_audio_data)} —á–∞–Ω–∫–æ–≤")
            
        except Exception as e:
            print(f"‚ùå –°–∏—Å—Ç–µ–º–Ω—ã–π –∑–≤—É–∫ –æ—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
    
    def start(self, region: dict = None, mic_device: int = None, record_system: bool = True):
        """–ù–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å"""
        if self.is_recording:
            return False
        
        if not region:
            print("‚ùå –û–±–ª–∞—Å—Ç—å –Ω–µ –≤—ã–±—Ä–∞–Ω–∞!")
            return False
        
        # –û—á–∏—Å—Ç–∫–∞
        self._video_frames = []
        self._mic_audio_data = []
        self._sys_audio_data = []
        self._stop_event.clear()
        
        # –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–ø–∏—é —Ä–µ–≥–∏–æ–Ω–∞ —Å int –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        self.monitor = {
            "left": int(region['left']),
            "top": int(region['top']),
            "width": int(region['width']),
            "height": int(region['height'])
        }
        self.mic_device = mic_device
        
        if record_system:
            self._loopback_device = self.get_loopback_device()
        else:
            self._loopback_device = None
        
        print(f"‚ñ∂Ô∏è –ó–∞–ø–∏—Å—å –æ–±–ª–∞—Å—Ç–∏: left={self.monitor['left']}, top={self.monitor['top']}, "
              f"width={self.monitor['width']}, height={self.monitor['height']}")
        
        self.is_recording = True
        
        # –ó–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫–æ–≤
        self._video_thread = threading.Thread(target=self._record_video, daemon=True)
        self._mic_thread = threading.Thread(target=self._record_microphone, daemon=True)
        self._sys_thread = threading.Thread(target=self._record_system_audio, daemon=True)
        
        self._video_thread.start()
        self._mic_thread.start()
        self._sys_thread.start()
        
        return True
    
    def stop(self) -> dict:
        """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å"""
        if not self.is_recording:
            return None
        
        print("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞...")
        self._stop_event.set()
        self.is_recording = False
        
        if self._video_thread:
            self._video_thread.join(timeout=3)
        if self._mic_thread:
            self._mic_thread.join(timeout=3)
        if self._sys_thread:
            self._sys_thread.join(timeout=3)
        
        return self._save_recording()
    
    def _save_recording(self) -> dict:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤–∏–¥–µ–æ —Å–æ –∑–≤—É–∫–æ–º"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = f"Meeting_{timestamp}"
        
        temp_video = str(self.output_dir / f"{base_name}_temp.avi")
        mic_audio_path = str(self.output_dir / f"{base_name}_mic.wav")
        sys_audio_path = str(self.output_dir / f"{base_name}_sys.wav")
        final_video = str(self.output_dir / f"{base_name}.mp4")
        
        result = {"video": None, "mic_audio": None, "sys_audio": None, "base_name": base_name}
        
        # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∏–¥–µ–æ (–±–µ–∑ –∑–≤—É–∫–∞)
        if self._video_frames:
            print(f"üíæ –í–∏–¥–µ–æ: {len(self._video_frames)} –∫–∞–¥—Ä–æ–≤...")
            h, w = self._video_frames[0].shape[:2]
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
            out = cv2.VideoWriter(temp_video, fourcc, self.fps, (w, h))
            for frame in self._video_frames:
                out.write(frame)
            out.release()
            print(f"   ‚úì –í—Ä–µ–º–µ–Ω–Ω–æ–µ –≤–∏–¥–µ–æ: {temp_video}")
        else:
            print("‚ö†Ô∏è –ù–µ—Ç –≤–∏–¥–µ–æ–∫–∞–¥—Ä–æ–≤!")
            return result
        
        # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—É–¥–∏–æ –ú–ò–ö–†–û–§–û–ù–ê
        if self._mic_audio_data:
            print(f"üíæ –ú–∏–∫—Ä–æ—Ñ–æ–Ω: {len(self._mic_audio_data)} —á–∞–Ω–∫–æ–≤...")
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –≤ –æ–¥–∏–Ω –º–∞—Å—Å–∏–≤
            audio_array = np.concatenate([chunk.flatten() for chunk in self._mic_audio_data])
            
            with wave.open(mic_audio_path, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(self.audio_rate)
                wf.writeframes(audio_array.tobytes())
            
            result["mic_audio"] = mic_audio_path
            print(f"   ‚úì –ú–∏–∫—Ä–æ—Ñ–æ–Ω: {mic_audio_path}")
        else:
            print("‚ö†Ô∏è –ù–µ—Ç –∞—É–¥–∏–æ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞!")
        
        # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –°–ò–°–¢–ï–ú–ù–´–ô –∑–≤—É–∫
        if self._sys_audio_data:
            print(f"üíæ –°–∏—Å—Ç–µ–º–Ω—ã–π –∑–≤—É–∫: {len(self._sys_audio_data)} —á–∞–Ω–∫–æ–≤...")
            audio_array = np.concatenate([chunk.flatten() for chunk in self._sys_audio_data])
            
            with wave.open(sys_audio_path, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(self.audio_rate)
                wf.writeframes(audio_array.tobytes())
            
            result["sys_audio"] = sys_audio_path
            print(f"   ‚úì –°–∏—Å—Ç–µ–º–Ω—ã–π –∑–≤—É–∫: {sys_audio_path}")
        else:
            print("‚ö†Ô∏è –ù–µ—Ç —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∑–≤—É–∫–∞")
        
        # 4. –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤–∏–¥–µ–æ + –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ FFmpeg
        try:
            print("üé¨ –û–±—ä–µ–¥–∏–Ω—è—é –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ...")
            
            import imageio_ffmpeg
            ffmpeg_path = imageio_ffmpeg.get_ffmpeg_exe()
            import subprocess
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ú–ò–ö–†–û–§–û–ù –¥–ª—è –∞—É–¥–∏–æ –≤ –≤–∏–¥–µ–æ (–ø—Ä–æ—Å—Ç–æ–µ —Ä–µ—à–µ–Ω–∏–µ)
            audio_for_video = result.get("mic_audio")
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∏ –º–∏–∫—Ä–æ—Ñ–æ–Ω –∏ —Å–∏—Å—Ç–µ–º–Ω—ã–π –∑–≤—É–∫ - –º–∏–∫—à–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ FFmpeg
            if result.get("mic_audio") and result.get("sys_audio"):
                print("   –ú–∏–∫—à–∏—Ä—É—é –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ FFmpeg...")
                mixed_audio = str(self.output_dir / f"{base_name}_mixed.wav")
                
                # FFmpeg –º–∏–∫—à–∏—Ä–æ–≤–∞–Ω–∏–µ
                mix_cmd = [
                    ffmpeg_path, '-y',
                    '-i', mic_audio_path,
                    '-i', sys_audio_path,
                    '-filter_complex', '[0:a][1:a]amix=inputs=2:duration=longest[aout]',
                    '-map', '[aout]',
                    '-ac', '1',
                    '-ar', str(self.audio_rate),
                    mixed_audio
                ]
                
                mix_proc = subprocess.run(mix_cmd, capture_output=True, text=True)
                if mix_proc.returncode == 0 and os.path.exists(mixed_audio):
                    audio_for_video = mixed_audio
                    print(f"   ‚úì –ú–∏–∫—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∞—É–¥–∏–æ —Å–æ–∑–¥–∞–Ω–æ")
                else:
                    print(f"   ‚ö†Ô∏è –ú–∏–∫—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å, –∏—Å–ø–æ–ª—å–∑—É—é –º–∏–∫—Ä–æ—Ñ–æ–Ω")
            
            if audio_for_video and os.path.exists(audio_for_video):
                # FFmpeg: –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤–∏–¥–µ–æ + –∞—É–¥–∏–æ
                cmd = [
                    ffmpeg_path, '-y',
                    '-i', temp_video,
                    '-i', audio_for_video,
                    '-c:v', 'libx264',
                    '-c:a', 'aac',
                    '-b:a', '192k',
                    '-shortest',
                    final_video
                ]
                
                print(f"   –°–æ–∑–¥–∞—é —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –≤–∏–¥–µ–æ...")
                proc = subprocess.run(cmd, capture_output=True, text=True)
                
                if proc.returncode == 0 and os.path.exists(final_video):
                    result["video"] = final_video
                    print(f"   ‚úÖ –í–∏–¥–µ–æ —Å–æ –∑–≤—É–∫–æ–º: {final_video}")
                    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                    if os.path.exists(temp_video):
                        os.remove(temp_video)
                    # –£–¥–∞–ª—è–µ–º mixed –µ—Å–ª–∏ —Å–æ–∑–¥–∞–≤–∞–ª–∏
                    mixed_path = str(self.output_dir / f"{base_name}_mixed.wav")
                    if os.path.exists(mixed_path):
                        os.remove(mixed_path)
                else:
                    print(f"   ‚ö†Ô∏è FFmpeg –æ—à–∏–±–∫–∞: {proc.stderr[:300] if proc.stderr else 'unknown'}")
                    final_avi = str(self.output_dir / f"{base_name}.avi")
                    import shutil
                    shutil.move(temp_video, final_avi)
                    result["video"] = final_avi
            else:
                final_avi = str(self.output_dir / f"{base_name}.avi")
                import shutil
                shutil.move(temp_video, final_avi)
                result["video"] = final_avi
                print(f"   ‚úÖ –í–∏–¥–µ–æ (–±–µ–∑ –∑–≤—É–∫–∞): {final_avi}")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            result["video"] = temp_video
        
        return result
