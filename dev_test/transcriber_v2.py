"""
Meeting Transcriber v2 ‚Äî –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤
- –ê—É–¥–∏–æ 1 (–º–∏–∫—Ä–æ—Ñ–æ–Ω) ‚Üí "–Ø"
- –ê—É–¥–∏–æ 2 (—Å–∏—Å—Ç–µ–º–Ω—ã–π) ‚Üí "–°–æ–±–µ—Å–µ–¥–Ω–∏–∫"
- –í—ã—Ö–æ–¥: —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á—ë—Ç —Å —Ç–∞–π–º–∫–æ–¥–∞–º–∏
"""
import os
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pathlib import Path
from datetime import datetime
import tempfile

import numpy as np
from scipy.io import wavfile

# Whisper
try:
    import whisper
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    print("‚ö†Ô∏è openai-whisper –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")


def format_timestamp(seconds: float) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏: MM:SS –∏–ª–∏ HH:MM:SS"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    
    if hours > 0:
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"
    return f"{minutes:02d}:{secs:02d}"


class MeetingTranscriber:
    """
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ç–æ—Ä –≤—Å—Ç—Ä–µ—á —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤
    """
    
    def __init__(self, model_name: str = "base"):
        self.model_name = model_name
        self.model = None
        
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å Whisper"""
        if not WHISPER_AVAILABLE:
            raise RuntimeError("openai-whisper –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
        
        if self.model is None:
            print(f"üì• –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å Whisper '{self.model_name}'...")
            self.model = whisper.load_model(self.model_name)
            print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        return self.model
    
    def transcribe_audio(self, audio_path: str, language: str = "ru") -> list:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å –∞—É–¥–∏–æ—Ñ–∞–π–ª —Å —Ç–∞–π–º–∫–æ–¥–∞–º–∏
        
        Returns:
            list of dict: [{"start": 0.0, "end": 2.5, "text": "–ü—Ä–∏–≤–µ—Ç"}]
        """
        self.load_model()
        
        print(f"üîÑ –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é: {audio_path}")
        
        result = self.model.transcribe(
            audio_path,
            language=language,
            task="transcribe",
            verbose=False
        )
        
        segments = []
        for seg in result.get("segments", []):
            segments.append({
                "start": seg["start"],
                "end": seg["end"],
                "text": seg["text"].strip()
            })
        
        return segments
    
    def transcribe_meeting(
        self,
        mic_audio_path: str = None,
        sys_audio_path: str = None,
        language: str = "ru"
    ) -> dict:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å –≤—Å—Ç—Ä–µ—á—É —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤
        
        Args:
            mic_audio_path: –ø—É—Ç—å –∫ WAV –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ ("–Ø")
            sys_audio_path: –ø—É—Ç—å –∫ WAV —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∑–≤—É–∫–∞ ("–°–æ–±–µ—Å–µ–¥–Ω–∏–∫")
            language: —è–∑—ã–∫ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
        
        Returns:
            dict: {"segments": [...], "full_text": "..."}
        """
        all_segments = []
        
        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º –º–∏–∫—Ä–æ—Ñ–æ–Ω ("–Ø")
        if mic_audio_path and os.path.exists(mic_audio_path):
            print("\nüé§ –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é –º–∏–∫—Ä–æ—Ñ–æ–Ω (–Ø)...")
            try:
                mic_segments = self.transcribe_audio(mic_audio_path, language)
                for seg in mic_segments:
                    seg["speaker"] = "–Ø"
                all_segments.extend(mic_segments)
                print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(mic_segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
        else:
            print(f"‚ö†Ô∏è –§–∞–π–ª –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {mic_audio_path}")
        
        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –∑–≤—É–∫ ("–°–æ–±–µ—Å–µ–¥–Ω–∏–∫")
        if sys_audio_path and os.path.exists(sys_audio_path):
            print("\nüîä –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é —Å–∏—Å—Ç–µ–º–Ω—ã–π –∑–≤—É–∫ (–°–æ–±–µ—Å–µ–¥–Ω–∏–∫)...")
            try:
                sys_segments = self.transcribe_audio(sys_audio_path, language)
                for seg in sys_segments:
                    seg["speaker"] = "–°–æ–±–µ—Å–µ–¥–Ω–∏–∫"
                all_segments.extend(sys_segments)
                print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(sys_segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
        else:
            if sys_audio_path:
                print(f"‚ö†Ô∏è –§–∞–π–ª —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∑–≤—É–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {sys_audio_path}")
        
        if not all_segments:
            print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏")
            return {
                "segments": [],
                "full_text": "(–ü—É—Å—Ç–æ - —Ä–µ—á—å –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞)"
            }
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        all_segments.sort(key=lambda x: x["start"])
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±–ª–∏–∑–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
        merged_segments = self._merge_segments(all_segments)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
        full_text = self._format_transcript(merged_segments)
        
        return {
            "segments": merged_segments,
            "full_text": full_text
        }
    
    def _merge_segments(self, segments: list, gap_threshold: float = 1.0) -> list:
        """
        –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –±–ª–∏–∑–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
        """
        if not segments:
            return []
        
        merged = []
        current = segments[0].copy()
        
        for seg in segments[1:]:
            # –ï—Å–ª–∏ —Ç–æ—Ç –∂–µ —Å–ø–∏–∫–µ—Ä –∏ –º–∞–ª–µ–Ω—å–∫–∏–π –ø—Ä–æ–º–µ–∂—É—Ç–æ–∫ - –æ–±—ä–µ–¥–∏–Ω—è–µ–º
            if (seg["speaker"] == current["speaker"] and 
                seg["start"] - current["end"] < gap_threshold):
                current["end"] = seg["end"]
                current["text"] += " " + seg["text"]
            else:
                merged.append(current)
                current = seg.copy()
        
        merged.append(current)
        return merged
    
    def _format_transcript(self, segments: list) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –≤ —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç"""
        lines = []
        for seg in segments:
            ts = format_timestamp(seg["start"])
            speaker = seg["speaker"]
            text = seg["text"]
            lines.append(f"[{ts}] {speaker}: {text}")
        
        return "\n".join(lines)
    
    def save_report(
        self,
        transcript: dict,
        output_path: str = None,
        video_path: str = None
    ) -> str:
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á—ë—Ç –æ –≤—Å—Ç—Ä–µ—á–µ
        
        Args:
            transcript: —Ä–µ–∑—É–ª—å—Ç–∞—Ç transcribe_meeting()
            output_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            video_path: –ø—É—Ç—å –∫ –≤–∏–¥–µ–æ (–¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞)
        
        Returns:
            –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            if video_path:
                base = Path(video_path).stem
                output_path = str(Path(video_path).parent / f"{base}_transcript.txt")
            else:
                output_path = f"Meeting_Report_{timestamp}.txt"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á—ë—Ç
        report_lines = [
            "=" * 60,
            "üìã –û–¢–ß–Å–¢ –û –í–°–¢–†–ï–ß–ï",
            f"üìÖ –î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            "=" * 60,
            "",
            "üìù –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø:",
            "-" * 40,
            "",
            transcript["full_text"],
            "",
            "-" * 40,
            f"üìä –í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(transcript['segments'])}",
            "=" * 60
        ]
        
        report_text = "\n".join(report_lines)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print(f"üìÑ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")
        return output_path


# ===== –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ =====
if __name__ == "__main__":
    print("\n" + "="*60)
    print("üî¨ –¢–ï–°–¢ MeetingTranscriber")
    print("="*60)
    
    transcriber = MeetingTranscriber(model_name="base")
    
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    print("""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

1. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –¥–≤—É–º—è –∞—É–¥–∏–æ–¥–æ—Ä–æ–∂–∫–∞–º–∏:
   result = transcriber.transcribe_meeting(video_path="meeting.mp4")
   transcriber.save_report(result, "report.txt")

2. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤:
   result = transcriber.transcribe_meeting(
       mic_audio_path="mic.wav",
       sys_audio_path="system.wav"
   )

3. –û–∂–∏–¥–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞:
   [00:00] –Ø: –í—Å–µ–º –ø—Ä–∏–≤–µ—Ç
   [00:03] –°–æ–±–µ—Å–µ–¥–Ω–∏–∫: –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ
   [00:08] –Ø: –ù–∞—á–Ω—ë–º –≤—Å—Ç—Ä–µ—á—É
""")
    
    # –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
    if WHISPER_AVAILABLE:
        print("\n–ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∞...")
        try:
            transcriber.load_model()
            print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    else:
        print("‚ùå Whisper –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    input("\n–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –≤—ã—Ö–æ–¥–∞...")
